{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"colab":{"name":"Exercise 1 - pre processing DKotsiras.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"59ov8ILAybVH","colab_type":"code","colab":{}},"source":["import requests\n","from bs4 import BeautifulSoup\n","import os\n","import bs4\n","import nltk\n","import codecs\n","import string\n","from readability.readability import Document"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dEtfGSTbybVO","colab_type":"code","colab":{}},"source":["r  = requests.get(\"https://en.wikipedia.org/wiki/Artificial_neural_network\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UuxFlE6lybVS","colab_type":"code","colab":{}},"source":["data = r.text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BA4QcuBRybVY","colab_type":"code","colab":{}},"source":["soup = BeautifulSoup(data , 'html5lib')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YhGNnNRNybVe","colab_type":"code","colab":{}},"source":["# This way we store only the information from the main div of the page and\n","# also removing some tags and their contents before moving to the preprocess steps\n","# we must create 2 folders in our system 1st folder: html 2nd folder: text\n","f = open('html/temp.html','w')\n","maindiv=soup.find_all('div','mw-parser-output')[0]\n","for sem in maindiv('semantics'):\n","    sem.decompose()\n","f.write(str(maindiv))\n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fO_VlIGAybVi","colab_type":"code","colab":{}},"source":["def evaluation(textdir):\n","    \"\"\"\n","    Uses the nltk.TaggedCorpusReader to answer the evaluation questions.\n","    \"\"\"\n","\n","    # Construct the corpus\n","    corpus    = nltk.corpus.TaggedCorpusReader(textdir, r'.*txt')\n","    sents=len(corpus.sents())\n","    # Construct stopwords\n","    stopwords = nltk.corpus.stopwords.words('english')\n","    stopwords.extend(list(string.punctuation))          # Remove punctuation\n","    stopwords.extend([\"''\", '``', \"'s\", \"n't\", \"'ll\"])  # Custom stopwords\n","    # Get the interesting words from corpus\n","    words     = [word.lower() for word in corpus.words() if word not in stopwords]\n","    # Count the words and tags\n","    tokens    = nltk.FreqDist(corpus.words())\n","    unigrams  = nltk.FreqDist(words)\n","    bigrams   = nltk.FreqDist(nltk.bigrams(words))\n","    tags      = nltk.FreqDist(tag for word, tag in corpus.tagged_words())\n","    # Eliminate stopwords\n","    for word in stopwords:\n","        unigrams.pop(word, None)\n","        bigrams.pop(word, None)\n","    # Enumerate the vocabulary and word count\n","    vocab     = len(tokens)            # The number of unique tokens\n","    count     = sum(tokens.values())   # The word count for the entire corpus\n","\n","    # Answer the evaluation questions\n","    print \"This corpus contains %i words with a vocabulary of %i tokens.\"  % (count, vocab)\n","    print \"This corpus contains %i sentences.\" % (sents)\n","    print \"The lexical diversity is %0.3f\" % (float(count) / float(vocab))\n","\n","    print \"The 5 most common tags are:\"\n","    for idx, tag in enumerate(tags.most_common(5)):\n","        print \"    %i. %s (%i samples)\" % ((idx+1,) + tag)\n","\n","    print \"\\nThe 10 most common unigrams are:\"\n","    for idx, tag in enumerate(unigrams.most_common(10)):\n","        print \"    %i. %s (%i samples)\" % ((idx+1,) + tag)\n","\n","    print \"\\nThe 10 most common bigrams are:\"\n","    for idx, tag in enumerate(bigrams.most_common(10)):\n","        print \"    %i. %s (%i samples)\" % ((idx+1,) + tag)\n","\n","    print \"\\nThere are %i nouns in the corpus\" % sum(val for key,val in tags.items() if key.startswith('N'))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"__JiluqXybVn","colab_type":"code","colab":{}},"source":["# Tags to extract as paragraphs from the HTML text\n","TAGS = [ 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']\n","\n","def preprocess(path):\n","    \"\"\"\n","    Opens a file, reads the contents - then performs the following tasks:\n","    1. Summarize the text with readability\n","    1. Gets all the target tags in the text\n","    2. Segments the paragraphs with the sent_tokenizer\n","    3. Tokenizes the sentences with the word_tokenizer\n","    4. Tags the sentences using the default pos_tagger\n","    It then yields a list of paragraphs whose data structure is a list of\n","    sentences that are tokenized and tagged.\n","    \"\"\"\n","\n","    with open(path, 'r') as f:\n","\n","        # Transform the document into a readability paper summary\n","        html = Document(f.read()).summary()\n","\n","        # Parse the HTML using BeautifulSoup\n","        soup = bs4.BeautifulSoup(html,\"lxml\")\n","\n","        # Extract the paragraph delimiting elements\n","        for tag in soup.find_all(TAGS):\n","\n","            # Get the HTML node text\n","            paragraph = tag.get_text()\n","\n","            # Sentence Tokenize\n","            sentences = nltk.sent_tokenize(paragraph)\n","            for idx, sentence in enumerate(sentences):\n","                # Word Tokenize and Part of Speech Tagging\n","                sentences[idx] = nltk.pos_tag(nltk.word_tokenize(sentence))\n","\n","            # Yield a list of sentences (the paragraph); each sentence of\n","            # which is a list of tuples in the form (token, tag).\n","            yield sentences\n","\n","\n","def transform(htmldir, textdir):\n","    \"\"\"\n","    Pass in a directory containing HTML documents and an output directory\n","    for the preprocessed text and this function transforms the HTML to a\n","    text corpus that has been tagged in the Brown corpus style.\n","    \"\"\"\n","    # List the target HTML directory\n","    for name in os.listdir(htmldir):\n","\n","        # Determine the path of the file to transform and the file to write to\n","        inpath  = os.path.join(htmldir, name)\n","        outpath = os.path.join(textdir, os.path.splitext(name)[0] + \".txt\")\n","\n","        # Open the file for reading UTF-8\n","        if os.path.isfile(inpath):\n","            with codecs.open(outpath, 'w+', encoding='utf-8') as f:\n","\n","                # Write paragraphs double newline separated and sentences\n","                # separated by a single newline. Also write token/tag pairs.\n","                for paragraph in preprocess(inpath):\n","                    for sentence in paragraph:\n","                        f.write(\" \".join(\"%s/%s\" % (word, tag) for word, tag in sentence))\n","                        f.write(\"\\n\")\n","                    f.write(\"\\n\")\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"euj9GERGybVt","colab_type":"code","colab":{}},"source":["transform('html', 'text')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2aFBbuoybVx","colab_type":"code","colab":{},"outputId":"a1b3ac85-9633-46ba-e820-68b549b0eb66"},"source":["evaluation('text/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["This corpus contains 22837 words with a vocabulary of 4586 tokens.\n","This corpus contains 1733 stentences.\n","The lexical diversity is 4.980\n","The 5 most common tags are:\n","    1. NNP (3574 samples)\n","    2. NN (3182 samples)\n","    3. JJ (1968 samples)\n","    4. . (1574 samples)\n","    5. IN (1482 samples)\n","\n","The 10 most common unigrams are:\n","    1. neural (246 samples)\n","    2. networks (191 samples)\n","    3. learning (174 samples)\n","    4. \\displaystyle (125 samples)\n","    5. network (110 samples)\n","    6. deep (88 samples)\n","    7. x (77 samples)\n","    8. j (64 samples)\n","    9. edit (62 samples)\n","    10. function (60 samples)\n","\n","The 10 most common bigrams are:\n","    1. (u'neural', u'networks') (125 samples)\n","    2. (u'neural', u'network') (61 samples)\n","    3. (u'\\\\displaystyle', u'\\\\textstyle') (57 samples)\n","    4. (u'artificial', u'neural') (37 samples)\n","    5. (u'x', u'\\\\displaystyle') (24 samples)\n","    6. (u'deep', u'learning') (22 samples)\n","    7. (u'j', u'\\\\displaystyle') (22 samples)\n","    8. (u'machine', u'learning') (20 samples)\n","    9. (u'cost', u'function') (16 samples)\n","    10. (u'f', u'x') (16 samples)\n","\n","There are 7908 nouns in the corpus\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hrSqrg7FybV3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9xWFp48IybV8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}